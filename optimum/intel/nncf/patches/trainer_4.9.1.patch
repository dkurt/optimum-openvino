diff --git a/trainer.py b/trainer2.py
index d6e0d51..0ede165 100644
--- a/trainer.py
+++ b/trainer.py
@@ -30,6 +30,10 @@ from logging import StreamHandler
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union

+
+from nncf.torch.nncf_network import NNCFNetwork
+from nncf.torch import create_compressed_model
+from optimum.intel.nncf import NNCFAutoConfig
 from tqdm.auto import tqdm


@@ -47,6 +51,7 @@ from .integrations import (  # isort: split

 import numpy as np
 import torch
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from packaging import version
 from torch import nn
 from torch.utils.data.dataloader import DataLoader
@@ -274,12 +279,26 @@ class Trainer:
         compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
         callbacks: Optional[List[TrainerCallback]] = None,
         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
+        nncf_config: NNCFAutoConfig = None,
     ):
         if args is None:
             output_dir = "tmp_trainer"
             logger.info(f"No `TrainingArguments` passed, using `output_dir={output_dir}`.")
             args = TrainingArguments(output_dir=output_dir)
         self.args = args
+
+        if nncf_config is not None:
+            nncf_config.auto_register_extra_structs(args, train_dataset, data_collator)
+            # TODO: restore compression state
+            # compression_state_file = os.path.join(model_name_or_path, NNCF_PT_STATE_NAME)
+            # if os.path.isfile(compression_state_file):
+            #     compression_state = torch.load(compression_state_file)
+            # else:
+            compression_state = None
+            self.compression_ctrl, model = create_compressed_model(
+                model, nncf_config, compression_state=compression_state
+            )
+
         # Seed must be set before instantiating the model when using model
         set_seed(self.args.seed)
         self.hp_name = None
@@ -510,7 +529,12 @@ class Trainer:
             return dataset
         if self._signature_columns is None:
             # Inspect model forward signature to keep only the arguments it accepts.
-            signature = inspect.signature(self.model.forward)
+
+            if isinstance(self.model, NNCFNetwork):
+                signature = inspect.signature(self.model.get_nncf_wrapped_model().forward)
+            else:
+                signature = inspect.signature(self.model.forward)
+
             self._signature_columns = list(signature.parameters.keys())
             # Labels may be named label or label_ids, the default data collator handles that.
             self._signature_columns += ["label", "label_ids"]
@@ -972,6 +996,10 @@ class Trainer:
                 find_unused_parameters = not getattr(model.config, "gradient_checkpointing", False)
             else:
                 find_unused_parameters = True
+
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.distributed()
+
             model = nn.parallel.DistributedDataParallel(
                 model,
                 device_ids=[self.args.local_rank],
@@ -1231,6 +1259,10 @@ class Trainer:
                     break

         for epoch in range(epochs_trained, num_train_epochs):
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.scheduler.epoch_step()
+                print(self.compression_ctrl.statistics().to_str())
+
             if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                 train_dataloader.sampler.set_epoch(epoch)
             elif isinstance(train_dataloader.dataset, IterableDatasetShard):
@@ -1275,9 +1307,10 @@ class Trainer:
                 ):
                     # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.
                     with model.no_sync():
-                        tr_loss += self.training_step(model, inputs)
-                else:
-                    tr_loss += self.training_step(model, inputs)
+                        curr_loss = self.training_step(model, inputs)
+                else:  # Just a comment to fix patching
+                    curr_loss = self.training_step(model, inputs)
+                tr_loss += curr_loss
                 self.current_flos += float(self.floating_point_ops(inputs))

                 # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps
@@ -1311,6 +1344,9 @@ class Trainer:
                             )

                     # Optimizer step
+                    if self.compression_ctrl is not None:
+                        self.compression_ctrl.scheduler.step()
+
                     optimizer_was_run = True
                     if self.deepspeed:
                         pass  # called outside the loop
@@ -1331,6 +1367,7 @@ class Trainer:
                     model.zero_grad()
                     self.state.global_step += 1
                     self.state.epoch = epoch + (step + 1) / steps_in_epoch
+                    self.state.curr_loss = curr_loss.cpu().detach().item()
                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)

                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
@@ -1426,6 +1463,13 @@ class Trainer:
             logs["loss"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
             logs["learning_rate"] = self._get_learning_rate()

+            if self.compression_ctrl is not None:
+                logs["compression_loss"] = self.compression_ctrl.loss().item()
+                compression_stats = self.compression_ctrl.statistics()
+                for key, value in prepare_for_tensorboard(compression_stats).items():
+                    logs["compression/statistics/{0}".format(key)] = value
+                print(compression_stats.to_str())
+
             self._total_loss_scalar += tr_loss_scalar
             self._globalstep_last_logged = self.state.global_step
             self.store_flos()
@@ -1778,6 +1822,9 @@ class Trainer:
         if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:
             # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`
             loss = loss / self.args.gradient_accumulation_steps
+        if self.compression_ctrl is not None:
+            compression_loss = self.compression_ctrl.loss()
+            loss += compression_loss

         if self.use_amp:
             self.scaler.scale(loss).backward()
@@ -1884,34 +1931,6 @@ class Trainer:
         elif self.args.should_save:
             self._save(output_dir)

-    def _save_tpu(self, output_dir: Optional[str] = None):
-        output_dir = output_dir if output_dir is not None else self.args.output_dir
-        logger.info(f"Saving model checkpoint to {output_dir}")
-
-        if xm.is_master_ordinal():
-            os.makedirs(output_dir, exist_ok=True)
-            torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
-
-        # Save a trained model and configuration using `save_pretrained()`.
-        # They can then be reloaded using `from_pretrained()`
-        xm.rendezvous("saving_checkpoint")
-        if not isinstance(self.model, PreTrainedModel):
-            if isinstance(unwrap_model(self.model), PreTrainedModel):
-                unwrap_model(self.model).save_pretrained(
-                    output_dir,
-                    save_config=self.args.should_save,
-                    state_dict=self.model.state_dict(),
-                    save_function=xm.save,
-                )
-            else:
-                logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
-                state_dict = self.model.state_dict()
-                xm.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))
-        else:
-            self.model.save_pretrained(output_dir, save_config=self.args.should_save, save_function=xm.save)
-        if self.tokenizer is not None and self.args.should_save:
-            self.tokenizer.save_pretrained(output_dir)
-
     def _save(self, output_dir: Optional[str] = None, state_dict=None):
         # If we are executing this function, we are the process zero, so we don't check for that.
         output_dir = output_dir if output_dir is not None else self.args.output_dir
@@ -1920,10 +1939,15 @@ class Trainer:
         # Save a trained model and configuration using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
         if not isinstance(self.model, PreTrainedModel):
-            if isinstance(unwrap_model(self.model), PreTrainedModel):
-                if state_dict is None:
-                    state_dict = self.model.state_dict()
-                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)
+            unwrapped_model = unwrap_model(self.model)
+            if isinstance(unwrapped_model, NNCFNetwork):
+                is_pretrained = isinstance(unwrapped_model.get_nncf_wrapped_model(), PreTrainedModel)
+            else:
+                is_pretrained = isinstance(unwrapped_model, PreTrainedModel)
+            if is_pretrained:
+                if state_dict is None:  # Just a comment line to fix patching
+                    state_dict = unwrapped_model.state_dict()
+                unwrapped_model.save_pretrained(output_dir, state_dict=state_dict)
             else:
                 logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
                 if state_dict is None:
@@ -1937,6 +1961,16 @@ class Trainer:
         # Good practice: save your training arguments together with the trained model
         torch.save(self.args, os.path.join(output_dir, "training_args.bin"))

+        path_to_onnx = os.path.join(output_dir, "ov_model.onnx")
+        self.compression_ctrl.export_model(path_to_onnx, input_names=["input_ids", "attention_mask"])
+
+        import subprocess
+
+        subprocess.run(
+            [sys.executable, "-m", "mo", "--input_model", path_to_onnx, "--output_dir", output_dir], check=True
+        )
+        os.remove(path_to_onnx)
+
     def store_flos(self):
         # Storing the number of floating-point operations that went into the model
         if self.args.local_rank != -1:
